{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "filepath1 = r\"2021MCM_ProblemC_ Images_by_GlobalID.xlsx\"\n",
    "filepath2 = r\"2021MCMProblemC_DataSet.xlsx\"\n",
    "\n",
    "data1 = pd.read_excel(filepath1)\n",
    "\n",
    "# 对文件后缀名进行更改\n",
    "for i in range(len(data1)):\n",
    "    if(data1[\"FileName\"][i].split(\".\")[-1] != \"jpg\"):\n",
    "        data1[\"FileName\"][i] = data1[\"FileName\"][i].split(\".\")[0]+\".jpg\"\n",
    "        data2 = pd.read_excel(filepath2)\n",
    "        data = pd.merge(data1, data2, on=\"GlobalID\")\n",
    "        \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.img_list = os.listdir(self.data_path)\n",
    "\n",
    "def __getitem__(self, index):\n",
    "    img_title = self.img_list[index]\n",
    "    # img_label = img_title.split('.')[0]\n",
    "    data = pd.merge(data1, data2, on=\"GlobalID\")\n",
    "    ind = data[data[\"FileName\"] == img_title].index.tolist()\n",
    "    if(data[\"Lab Status\"][ind[0]] == \"Unprocessed\"):\n",
    "        img_label = np.array([0])\n",
    "    elif(data[\"Lab Status\"][ind[0]] == \"Unverified\"):\n",
    "        img_label = np.array([1])\n",
    "    elif(data[\"Lab Status\"][ind[0]] == \"Positive ID\"):\n",
    "        img_label = np.array([2])\n",
    "    else:\n",
    "        img_label = np.array([3])\n",
    "        img_path = os.path.join(self.data_path, img_title)\n",
    "        img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "    # print(img.shape)\n",
    "    return img, img_label\n",
    "\n",
    "\n",
    "def __len__(self):\n",
    "    return len(self.img_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = r'D:\\Spyder\\MCM_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(train_path)\n",
    "# train_dataset=dataset.\n",
    "train_size = int(dataset.__len__() * 0.8)\n",
    "test_size = dataset.__len__() - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "\n",
    "# Input:[Batch, Channels, Height, Width]\n",
    "for batch_idx, data in enumerate(train_loader):\n",
    "    input, tar = data\n",
    "    # print(type(input),'\\t',input.shape)\n",
    "    input = input.permute(0, 3, 1, 2)\n",
    "    model = Net()\n",
    "    output = model(input)\n",
    "    _, predicted = torch.max(output.data, dim=1)\n",
    "    # print((input),'\\t',input.shape)\n",
    "    print(predicted.size()[0])\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from time import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchsnooper\n",
    "import torch.optim as optim\n",
    "# from Dataset_make import MyDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prepare dataset\n",
    "batch_size = 64\n",
    "path = r'D:\\Spyder\\Data_cut'\n",
    "dataset = MyDataset(path)\n",
    "train_size = int(dataset.__len__() * 0.8)\n",
    "test_size = dataset.__len__() - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset,shuffle=True,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset,shuffle=False,batch_size=batch_size)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Input:Batch_size,3,600,450\n",
    "        self.conv1 = torch.nn.Conv2d(3,10 , kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv3=torch.nn.Conv2d(20,5,kernel_size=3)\n",
    "        self.conv4=torch.nn.Conv2d(5,5,kernel_size=3)\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(350, 4)\n",
    "        # @torchsnooper.snoop()\n",
    "        \n",
    "    def forward(self, x):\n",
    "    # 定义了每次执行的计算步骤。在所有的子类中都需要重写这个函数。\n",
    "        # Flatten data from (n, 1, 28, 28) to (n, 784)\n",
    "        batch_size = x.size(0)\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.pooling(self.conv1(x)))\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.pooling(self.conv3(x)))\n",
    "        # print(x.size())\n",
    "        x = F.relu(self.pooling(self.conv4(x)))\n",
    "        # print(x.size())\n",
    "        x = x.view(batch_size, -1) # flatten\n",
    "        # print(x.size())\n",
    "        x = self.fc(x)\n",
    "        # print(x.size())\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "# (print(next(model.parameters()).device))\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        inputs, target = data\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        inputs=inputs.permute(0,3,1,2)\n",
    "        inputs=inputs.cuda()\n",
    "        # print(inputs.is_cuda)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target=target.cuda()\n",
    "        # print(target.is_cuda)\n",
    "        optimizer.zero_grad()#将module中的所有模型参数的梯度设置为0.\n",
    "        # forward + backward + update\n",
    "        outputs = model(inputs)\n",
    "        temp=[]\n",
    "    for i in range(len(target)):\n",
    "        temp.append(target[i].item())\n",
    "        # print(len(temp))\n",
    "        temp=torch.LongTensor(temp)\n",
    "        temp=temp.cuda()\n",
    "        # print(temp.shape)\n",
    "        loss = criterion(outputs, temp)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if batch_idx % 10 ==9:\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss ))\n",
    "        running_loss = 0.0\n",
    "    torch.save(model.state_dict(), 'cnn_50.pkl')\n",
    "    del inputs, target, outputs, loss\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            inputs=inputs.permute(0,3,1,2)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            inputs=inputs.cuda()\n",
    "            target=target.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += target.size(0)\n",
    "        temp=[]\n",
    "        for i in range(len(target)):\n",
    "            temp.append(target[i].item())\n",
    "            temp=torch.LongTensor(temp)\n",
    "            temp=temp.cuda()\n",
    "            correct += (predicted == temp).sum().item()\n",
    "            print('Accuracy on test set: %d %% [%d/%d]' % (100 * correct / total, correct, total))\n",
    "            return correct / total\n",
    "    if __name__ == '__main__':\n",
    "        auc=[]\n",
    "    begin_time = time()\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "    auc.append(test())\n",
    "    end_time = time()\n",
    "    run_time = end_time - begin_time\n",
    "    plt.plot(auc)\n",
    "    font={'family' : 'Times New Roman','weight' : 'normal','size' : 30}\n",
    "    font2 ={'family' : 'Times New Roman','weight' : 'normal','size' : 20}\n",
    "    plt.xlabel(\"Epoch\",font2)\n",
    "    plt.ylabel(\"Auc\",font2)\n",
    "    plt.title(\"AUC Line\",font)\n",
    "    plt.grid\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net()\n",
    "for batch_idx, data in enumerate(train_loader):\n",
    "    inputs, target = data\n",
    "    inputs = inputs.type(torch.FloatTensor)\n",
    "    inputs=inputs.permute(0,3,1,2)\n",
    "    outputs=model(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33a9389eee663d4719950dddcee79ef48c2e0cc0c03addadf61dc83d39fbe88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
